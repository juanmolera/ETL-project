{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1: Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd # data manipulation and dataframes\n",
    "import numpy as np # arrays manipulation and mathematical operations\n",
    "\n",
    "# Web scraping with Selenium\n",
    "from selenium import webdriver # drives a browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager # installs and keeps the chrome driver updated\n",
    "from selenium.webdriver.common.keys import Keys # simulates keyboard keys\n",
    "from selenium.webdriver.chrome.options import Options # configures the chrome driver as incognito mode or maximizes the window\n",
    "\n",
    "# Runtime management\n",
    "from time import sleep # delay between code executions\n",
    "\n",
    "# Python configuration\n",
    "import warnings # ignores python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# HTML parsing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Regular expresions\n",
    "import re\n",
    "\n",
    "# Datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of the chrome driver\n",
    "ops = Options()\n",
    "ops.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "\n",
    "# Hides you as a robot\n",
    "ops.add_experimental_option('useAutomationExtension', False)\n",
    "ops.add_argument('--start-maximized') # start maximized\n",
    "ops.add_argument('user.data-dir=selenium') # saves cookies\n",
    "ops.add_argument('--incognito') # incognito window"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL extraction with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [] # products url list\n",
    "flag1 = 0 # breaks category loop\n",
    "flag2 = 0 # breaks subcategory loop\n",
    "total_categories = 100 # exceeds real value # wip\n",
    "total_subcategories = 100 # exceeds real value # wip\n",
    "\n",
    "# Opens driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Get supermarket url\n",
    "driver.get('https://www.dia.es/') \n",
    "sleep(3) # waits for cookies pop-up to load\n",
    "\n",
    "# Accepts cookies\n",
    "driver.find_element('css selector', '#onetrust-accept-btn-handler').click()\n",
    "sleep(2)\n",
    "\n",
    "# Clicks on products menu\n",
    "driver.find_element('css selector', '#app > div > div > div > div.home-view__header > div.dia-header > div.dia-header__section.dia-header__section--start > div > div > button').click()\n",
    "sleep(2)\n",
    "\n",
    "# Loop to get all subcategory urls inside each category\n",
    "for category in range(1, total_categories):\n",
    "\n",
    "    while 1:\n",
    "\n",
    "        try:\n",
    "            \n",
    "            # Cliks category\n",
    "            driver.find_element('xpath', f'//*[@id=\"app\"]/div/div/div/div[1]/div[1]/div[1]/div/div[2]/div/div/div[2]/ul/li[{category}]/a').click()\n",
    "            flag2 = 0 # resets flag2 after each category iteration\n",
    "\n",
    "            for subcategory in range (1, total_subcategories):\n",
    "\n",
    "                while 1:\n",
    "\n",
    "                    try:\n",
    "                        \n",
    "                        # Appends subcategory url to urls list\n",
    "                        urls.append(driver.find_element(\"xpath\", f'//*[@id=\"app\"]/div/div/div/div[1]/div[1]/div[1]/div/div[2]/div/div/div[2]/ul/li[{category}]/ul/div[{subcategory}]/a').get_attribute(\"href\"))\n",
    "                        sleep(0.1) # delay between subcategories\n",
    "                        break\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        flag2 = 1 # if the append action fails, flag2 is raised and it breaks\n",
    "                        break\n",
    "                    \n",
    "                if flag2:\n",
    "                    \n",
    "                    break\n",
    "            \n",
    "            sleep(1) # delay between categories\n",
    "            break\n",
    "        \n",
    "        except:\n",
    "\n",
    "            flag1 = 1 # if the click action fails, flag1 is raised and it breaks\n",
    "            break\n",
    "\n",
    "    if flag1:\n",
    "\n",
    "        break\n",
    "\n",
    "driver.quit() # closes driver when finish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product information extraction with Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wanted information of each product\n",
    "scraping_result = {'supermarket': [],\n",
    "                'category': [],\n",
    "                'subcategory': [],\n",
    "                'name': [], \n",
    "                'price': [], \n",
    "                'reference_price': [],\n",
    "                'reference_unit': [],\n",
    "                'insert_date': []}\n",
    "\n",
    "total = 0 # total products count\n",
    "page_buttons = [] # page buttons inside subcategories\n",
    "\n",
    "# Opens driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# For each previously extracted subcategory url\n",
    "for u in urls:\n",
    "\n",
    "        # Get subcategory url\n",
    "        driver.get(u)\n",
    "        sleep(3) # waits for cookies pop-up to load\n",
    "\n",
    "        # Accepts cookies\n",
    "        try:\n",
    "\n",
    "                driver.find_element('css selector', '#onetrust-accept-btn-handler').click()\n",
    "                sleep(1)\n",
    "\n",
    "        except:\n",
    "\n",
    "                pass\n",
    "\n",
    "        # Scrolls down\n",
    "        Y = 1200 # by pixel\n",
    "        driver.execute_script(f'window.scrollTo(0, {Y})') \n",
    "        sleep(1)\n",
    "\n",
    "        # Soup creation by parsing html information\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Aux lists with parsing info\n",
    "        cat = soup.find(\"span\", {\"class\": \"plp-breadcrumb__first-level-category\"}) # category\n",
    "        sub = soup.find(\"span\", {\"class\": \"plp-breadcrumb__second-level-category\"}) # subcategory\n",
    "        pro = soup.find_all(\"p\", {\"class\": \"search-product-card__product-name\"}) # products\n",
    "        pri = soup.find_all(\"p\", {\"class\": \"search-product-card__active-price\"}) # prices\n",
    "        kg = soup.find_all(\"p\", {\"class\": \"search-product-card__price-per-unit\"}) # price per unit\n",
    "\n",
    "        # Number of products in the current category\n",
    "        try:\n",
    "\n",
    "                num = soup.find('span', {\"class\": \"plp-breadcrumb__total-items\"}).text\n",
    "\n",
    "        except:\n",
    "\n",
    "                num = 0\n",
    "\n",
    "        # Regex to obtain the numeric value and increment total variable\n",
    "        total += int(re.findall('\\d+', str(num))[0])\n",
    "\n",
    "        # Cleans and appends scraped results to the dictionary\n",
    "        for p in pro:\n",
    "                scraping_result['supermarket'].append('dia-es') # wip\n",
    "                scraping_result['category'].append(cat.text.lower().replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u').replace('ñ', 'n').replace(',', '').replace(\" \", \"_\"))\n",
    "                scraping_result['subcategory'].append(sub.text.lower().replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u').replace('ñ', 'n').replace(',', '').replace(\" \", \"_\"))\n",
    "                scraping_result['name'].append(p.text)\n",
    "                scraping_result['insert_date'].append(datetime.today().strftime('%Y-%m-%d'))\n",
    "                        \n",
    "        for i in pri:\n",
    "                scraping_result['price'].append(i.text.rstrip('\\xa0€').replace(',', '.'))\n",
    "        \n",
    "        for k in kg:\n",
    "                lst = k.text.split('\\xa0€/')\n",
    "                scraping_result['reference_price'].append(lst[0].lstrip(' (').replace(',', '.'))\n",
    "                scraping_result['reference_unit'].append(lst[1].rstrip(') ').lower())\n",
    "        \n",
    "        # Finds buttons if exist\n",
    "        if bool(soup.find_all('a', {'class': 'pagination-button__page--links'})):\n",
    "\n",
    "                buttons = soup.find_all('a', {'class': 'pagination-button__page--links'})\n",
    "\n",
    "        else:\n",
    "                pass\n",
    "        \n",
    "        # Repeats all the process if buttons\n",
    "        if buttons:\n",
    "\n",
    "                # For each button (button 1 is already clicked)\n",
    "                for button in range(2, int(buttons[-1].text)+1):\n",
    "\n",
    "                        # Clicks button\n",
    "                        try:\n",
    "\n",
    "                                driver.find_element('xpath', f'//*[@id=\"app\"]/div/div/div/div[2]/div[2]/div[3]/div[2]/div/div/div/div[{button}]/a').click()\n",
    "                                sleep(1)\n",
    "                        \n",
    "                        except:\n",
    "                                \n",
    "                                # If there are more than 5 buttons\n",
    "                                # Clicks right arrow\n",
    "                                try:\n",
    "\n",
    "                                        driver.find_element('xpath', f'//*[@id=\"app\"]/div/div/div/div[2]/div[2]/div[3]/div[2]/div[1]/a[2]').click()\n",
    "                                        sleep(1)\n",
    "\n",
    "                                except:\n",
    "\n",
    "                                        pass\n",
    "\n",
    "                                pass\n",
    "                        \n",
    "                        # Same process as before\n",
    "                        driver.execute_script(f\"window.scrollTo(0, {Y})\") \n",
    "                        sleep(1)\n",
    "\n",
    "                        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                        prod = soup.find_all(\"p\", {\"class\": \"search-product-card__product-name\"})\n",
    "                        precio = soup.find_all(\"p\", {\"class\": \"search-product-card__active-price\"})\n",
    "                        kilo = soup.find_all(\"p\", {\"class\": \"search-product-card__price-per-unit\"})\n",
    "                        \n",
    "                        for p in prod:\n",
    "                                scraping_result['supermarket'].append('dia-es')\n",
    "                                scraping_result['category'].append(cat.text.lower().replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u').replace('ñ', 'n').replace(',', '').replace(\" \", \"_\"))\n",
    "                                scraping_result['subcategory'].append(sub.text.lower().replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u').replace('ñ', 'n').replace(',', '').replace(\" \", \"_\"))\n",
    "                                scraping_result['name'].append(p.text)\n",
    "                                scraping_result['insert_date'].append(datetime.today().strftime('%Y-%m-%d'))\n",
    "\n",
    "                        for o in precio:\n",
    "                                scraping_result['price'].append(o.text.rstrip('\\xa0€').replace(',', '.'))\n",
    "                        \n",
    "                        for k in kg:\n",
    "                                lst = k.text.split('\\xa0€/')\n",
    "                                scraping_result['reference_price'].append(lst[0].lstrip(' (').replace(',', '.'))\n",
    "                                scraping_result['reference_unit'].append(lst[1].rstrip(') ').lower())\n",
    "                        \n",
    "        botones = [] # resets buttons\n",
    "\n",
    "driver.quit() # closes driver\n",
    "\n",
    "# Export scraping result to csv\n",
    "df = pd.DataFrame(scraping_result)\n",
    "today_date = datetime.today().strftime('%Y-%m-%d %H-%M-%S')\n",
    "df.to_csv(f'../scrap({today_date}).csv', index = True, sep = ',')\n",
    "\n",
    "# Checks performance\n",
    "print(f'Scraped products: {len(scraping_result[\"name\"])} of {total}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
